[["index.html", "Andes Phylogenomics Workshop Chapter 1 About this workshop", " Andes Phylogenomics Workshop Rachel Schwartz and Rosana Zenil-Ferguson 2023-06-21 Chapter 1 About this workshop The evolutionary tree of life is fundamental to our understanding of the natural world. This workshop will go through all of the steps of research in understanding the evolutionary history of several groups of Andean plants. We will participate in field collections of plants and processing of samples for collections. In real research projects, the species and collections should be considered carefully with an expectation of the potential for phylogenetic results and evolutionary hypotheses. Researchers should create voucher specimens to preserve the samples long-term, as well as appropriate samples for DNA extraction. Samples should be recorded and given a unique identifier, in order to connect the specimen to the sequencing data. In this workshop we will not have an opportunity to experience the expected next step of sample processing, DNA extraction, or sequencing. Contemporary sequencing is often completed by an outside facility, although we will discuss options as part of this workshop. We will continue with the analysis of sequence data, particularly target-capture data from genome sequencing. As part of this we will learn basic computing on the command line and some approaches to writing code in the bash shell and R. This will allow us to conduct data analysis that can apply to larger scale work than can be conducted on a laptop. As part of the analysis of different datasets, we will see how in many cases we draw different conclusions from different datasets and how we examine phylogenies. We will then use these phylogenies to conduct comparative analyses to understand how phylogenies inform our understanding of trait evolution. Finally, we will ask you to present the results of your analyses. "],["intro.html", "Chapter 2 Introduction 2.1 Introduction 2.2 Case Studies 2.3 Course Objectives", " Chapter 2 Introduction 2.1 Introduction Biological research is turning to genetic research methods for a deeper look into the biological factors that encode various traits. We can use genetic techniques to delimit species, define populations, understand reproductive patterns systems, and answer many other interesting biological questions. This program is set in Colombia, where students will learn how field research is conducted, participate in sample collection, and then interpret genetic data from these species to examine their relationships and understand how to accurately conduct analyses in these areas. This program will provide an introduction to next-generation sequencing technology to biologists, who will gain not only the skills requisite for field research but the technical know-how to employ genetic research tools. 2.2 Case Studies In this course, we will focus on three specific cases We will use data from species in the family Campanulaceae. We will break into groups and each examine a subset of these data. We will examine different datasets using different approaches to understand how individual datasets contribute to phylogeny estimation. This tutorial steps through the basic process of analyses in evolutionary biology. We use genome sequence data to build phylogenetic trees. We then use these phylogenies and trait data to understand… 2.3 Course Objectives The goals of this course are to give participants advanced training in techniques important to the collection and analysis of plant evolution. The course has the following broad objectives: To engage in both independent and team-based data collection To teach sample collection techniques for plants Learn to examine sequence data with code Learn to build phylogenies from large datasets Basic ability to look at other types of data Be able to use comparative approaches to look at evolutionary questions based on phylogenies "],["data-and-analysis.html", "Chapter 3 Data and analysis 3.1 Genomic data 3.2 Phylogenies 3.3 Comparative analyses", " Chapter 3 Data and analysis 3.1 Genomic data By definition genomes are large. The human genome is 3.2 billion base pairs. Plants can be even larger. If you were to go through and compare two plant genomes by hand to discover the differences between them it would take a while. To add to this complexity, genomes don’t come off the sequencer as a 3.2 billion base pair sequence, but as millions of small fragments. Think of this sequencing method as tossing a book (your genome) into a paper shredder. That means first you have to compare each fragment to a “reference genome” (like the real book) and then figure out where your genome differs from the reference. This is definitely a process you don’t want to do by hand! A computer (at least a big one with lots of computing power) can help us out by automating all these comparisons. That means that before we can identify how the sequence of two plants differ we need to build some skills to communicate with the server that we are using to work with our data. 3.2 Phylogenies Different analyses produce different trees. Different datasets produce different trees. Rapid diversification makes analyses challenging. Divergence dates can be estimated on trees with calibrations. 3.3 Comparative analyses Understand models of character evolution and apply simple PCMs to existing datasets. Calculating trait evolution. "],["computational-skills.html", "Chapter 4 Computational skills 4.1 Log in to the server 4.2 Load and set up data 4.3 Analyses", " Chapter 4 Computational skills The high-performance computer we’ll use for our data doesn’t have the graphical interface you usually use (like a PC or Mac). That means you have to type in your commands - you can’t point and click. This actually has a hidden benefit because it’s easy to write down what you did in a line of text rather than having to try to explain where to click on each step. Let’s start by learning how to access our server and run commands without clicking. 4.1 Log in to the server For this duration of this workshop you will have access to a server at the University of Rhode Island. Go to rstudio.uri.edu and log in with the first part of your email address as both the username and password. 4.2 Load and set up data We will be using a dataset to practice that will look a lot like the data you plan to collect. By the end of this workshop you should be able to examine your own data in a similar way. Our practice dataset comes from Onstein et al. 2019, which you can find at https://onlinelibrary.wiley.com/doi/10.1111/jbi.13552. We have uploaded this paper to the server you are working on and we will show you how to access it there shortly. Let’s start by getting organized. We will use both R code and bash/shell scripts in this workshop. Because RStudio provides a mechanism for setting up projects we will use this to stay organized. In the RStudio File menu select New Project - New Directory - New Project. Give the project a name (e.g. evolution_workshop). For this and all future work ensure that names never include spaces. Select R 4.1.2 Create Project By creating a new project you have created a folder for your work. When using projects whenever you come back to this project you will return to the same setup. This allows you to switch projects with different folders and open files. 4.3 Analyses Begin your work by making a script. This is where you will keep track of all of your analytical commands. From the File menu select New File - R script. To analyze data in R we need to load some helper “libraries”. A common library for analysis is the tidyverse, which wraps multiple libraries made by RStudio. For details and cheatsheets see https://www.tidyverse.org/ . We will also use a library that allows us to read Excel files. Put the following in your script then click the Run icon. library(readxl) library(tidyverse) Now we will write a command to read the Excel file with the data from the paper. The command we use is read_xlsx. This command takes two arguments in parentheses. The first is the “path” to the data. We will discuss paths more extensively later. For now use the provided argument. The second argument is the Excel sheet or tab we want to read. Additionally, when we read the file into memory we assign the information to a “variable”. You can think of the variable as a box to contain the information. onstein &lt;- read_xlsx(&#39;../../shared/AndesWorkshop2023/Onstein_data.xlsx&#39;, sheet = &#39;Matrix for analysis&#39;) When you run this command you will see the data saved in the environment. When you click on the variable in the environment you will be able to view the data in tabular format. Before we can do anything with the data, we need to make sure that the computer understands the data the way we expect. Click on the arrow next to the variable name in the environment. Note that all of the columns of data are listed as “chr”. This means that the information is viewed as characters. Some of our data is actually numeric and we must convert it in order to analyze it correctly. To view the column names we use the colnames command and provide the dataset as the argument. We access a single column by specifying the dataset name, followed by a $ followed by the column name. For example: onstein$Log_Fruit_length_avg We assign a “corrected” version of the data to this variable. For example: onstein$Log_Fruit_length_avg &lt;- as.numeric(onstein$Log_Fruit_length_avg) Repeat this analysis to ensure that all columns match your expectations. Now we can visualize the data. We use the command ggplot and provide the data and the independent and dependent variables we expect on the graph. For example, we can examine the relationship between seed length and width. ggplot(onstein, aes(Log_Seed_width_avg, Log_Seed_length_avg)) Note that the variables are provided in an extra function aes. If you run this command it will generate an empty plot. To plot the points you have to “add” a “layer” on this base of the plot style. In this case we use the geom_point function to add the data as a scatter plot. ggplot(onstein, aes(Log_Seed_width_avg, Log_Seed_length_avg)) + geom_point() Do you notice an apparent relationship between these variables? We can view this more clearly by adding a regression line to our plot. geom_smooth(method = &quot;lm&quot;, se = FALSE) The paper from which these data are drawn found “syndromes” in frugivory-related traits such that fruits and seeds with dispersal by particular mechanisms (e.g. mammals, birds, and bats) have common trait values. Thus, we can see relationships among traits as above. The paper describes: “mammal syndromes of few, syncarpous fruits with many seeds, large fruits, and large seeds” “bird syndromes of many, bright-coloured, small fruits with few, small seeds and long stipes” “bird syndromes of dehiscent fruits with small seeds” “bat syndromes of dull-coloured, cauliflorous fruits” Try plotting some relationship on your own. To get you started, you can plot Log_Fruit_length_avg v Log_Seed_number_avg and Log_Fruit_length_avg v. Log_Seed_length_avg. Try others as well. If you tried plotting with a character variable on the x axis you might find the results difficult to look at. You can try using a boxplot with geom_boxplot() or other plot type instead. Check out the ggplot cheatsheet for ideas: https://posit.co/resources/cheatsheets/?type=posit-cheatsheets&amp;_page=2/ When you have several interesting relationships it’s advisable to confirm that the relationship is statistically significant. We can examine relationships with a linear model using the lm function. For example: lm(Log_Seed_length_avg ~ Log_Seed_width_avg, data = onstein) Note that the format of the arguments is slightly different than in a ggplot. The output of this function does not provide information without additional work. Assign the output to a new variable. Then provide this variable as the argument to the summary command. Examine the p and r-squared value. Do these match your observation from the graph? Note: make sure your variable names are informative. Repeat this process for your other graphs. Make sure you understand the biological interpretation of your results. Discuss with you instructor as needed. "],["computational-skills-1.html", "Chapter 5 Computational skills 5.1 Basic navigation", " Chapter 5 Computational skills 5.1 Basic navigation Click on the Terminal tab to access the command line interface, which we’ll be using for our genomic analyses. To see the list of files on this computer in your “home” directory do the following: ls This folder might be empty because you haven’t put anything here. We will begin our work using the command line or shell. Select the Terminal tab. First, make a directory to put your scripts in inside your workshop folder. We’ll write these scripts in a little while, but basically a script is a list of commands that you will give the computer to do all the steps to analyze your data. Start by checking which folder you are in using the following command. pwd If you are already in your project folder you may skip the next step. Otherwise you must move into that directory. To change directories you need two things - the command to change directory and the argument that specifies which directory to move into. cd evolution_workshop To make a directory you again need a command (make directory) and an argument (the name). mkdir scripts Now we can move into that folder. cd means change directory so we’ll use that a lot to open different folders. This command also has an argument. cd scripts Now to get back to your home folder you can’t cd and give it a folder name because the computer will look in your current folder (scripts) for another folder. Instead you need to tell the computer to “move one level up”, going outside the current folder to the one containing it. cd .. To practice, repeat this process but make a directory called results. Now list the files in your current home directory again. Do you see the two folders you just made? If you want to check out what is in these folders (they are currently empty but we will add to them later) you have two options. First, you can change directory with the cd command and list the contents of the folder with ls. Think about these folder just like a filing cabinet. You started off in your home folder and now you are opening the results folder. Alternatively, list the contents of a folder by given ls the folder name as an argument (e.g. ls scripts). We will learn lots more commands as we work through our data. Adapted from http://swcarpentry.github.io/shell-novice/ "],["getting-your-data.html", "Chapter 6 Getting your data 6.1 Repeating analyses", " Chapter 6 Getting your data For this workshop we will be using genome sequence data from a group of Andean plants. For starters we will take a peak at these data so you know what it looks like before we use standard programs to analyze it. The sequence data is in a shared folder on this server. Relative to your workshop folder you can find the data at ../../shared/AndesWorkshop2023/. If you cd to that directory and list the contents you can see folders containing files of data. Sequence files for samples can be found in different folders labeled by taxon set. Start by going to the folder for your assigned taxon set and listing its contents. You should see two folders. Go into the one for your assigned dataset. You generally do not want to view sequence data files because they can be large and are rather hard to read. If you want to see how large use ls -lh. You are already familiar with the ls command for listing the contents of folders. The dash followed by additional letters are called flags. The l flag indicates that we are listing the contents in long form to provide additional information. The h flag means we’ll view the data in human readable format (i.e. with size given in Kb or Mb). Raw data is in the form of zipped fastq files, which are generated by a sequencer. You can tell this by the very last bit of the name of the files, which is .fastq.gz or .fq.gz (just like you might see Word files labeled as .docx). Just as a note, you often need to do some cleanup of the data that comes of the sequencer so you only have high quality data. We have done this step for you already. Take a look at a little bit of one file. The less command lets you scroll through the (very large) file. zless is the less command that works on zipped files. Use the following command replacing [] with a specific filename. zless [].fastq.gz Fastq files contain sets of four lines. The first line is the name of the sequence. The second line is the sequence. The fourth line is the quality of the sequence. As you scroll through you should see many sets of four (one for each sequence). Once you are done scrolling use q (for quit) to get back to your prompt. Before we look at our large files we are going to practice on a small example file. Move back to the main AndesWorkshop2023 folder. List the contents and notice the example.fq file. For this example I have unzipped it. You can view the entire contents of the file using the command cat. cat example.fq Alternatively you can use the command head to view the first few lines of the file. head example.fq We can also get a sense of the amount of data by counting the number of lines using the word count command wc. wc example.fq This command tells you the number of lines, words, and characters. If you want to focus just on the number of lines you should use the l flag. wc -l example.fq If we only want to get the names of the sequences we can search for lines that match a particular pattern and print just those. We search with the command grep. The pattern we are searching for is lines that start with @. And then we provide the name of the file. grep &#39;^@&#39; example.fq Because this is a small example file we are able to print all the header lines without it being overwhelming. However, with our larger files we don’t want to do this. However, we know that the head command allows us to view just the first few lines of our output. Now we’ll combine those two commands using a pipe (|). First we enter the grep command (with it’s two arguments) then we “pipe” the output to the head command rather than printing it to the screen. grep &#39;^@&#39; example.fq | head Notice that the head command does not have arguments in this case because it accepts the output from the grep command. The head command accepts a flag with the number of lines you want to print. For example head -4 will print the first four lines. Your first challenge in this course is to combine the information that you have learned to get the number of header lines in one of your zipped fastq files (using zgrep). 6.1 Repeating analyses One approach to analyzing multiple files in the same way is to copy your code and edit each copy to include the name of a particular file. That might be feasible (if slow) for the 10 files you are working with here, but it is inconvenient for the many files you might work with on a larger genomics project. Instead, we can make a list of the files we want to work with and tell the computer to repeat the analysis on each file. In the prior challenge you figured out what analysis you want to do (list the number of header lines). Now we need to learn how to repeat this. In most programming languages, including our command line, we use a loop to go through a list and repeat that command. On the command line the format to do this looks like for thing in list_of_things do command $thing done We start with for to indicate that we are about to make a loop. Each time the loop runs (called an iteration), an item in the list is assigned in sequence to the variable thing, and the commands inside the loop are executed. Think of a variable as a box that holds the item we are working with. For example, if we are working with a list of files then the first time through the loop thing is the first file name in the list, the second time through thing is the second name, and so on. This allows us to repeat our command for each item in the list using the same variable name. Inside the loop, we indicate the variable by putting $ in front of it. The $ tells the shell interpreter to treat the variable as a variable name and substitute its value in its place, rather than treat it as text or an external command. Start by figuring out what goes on the third line of the loop Now figure out what goes on the first line (i.e. your list) When you run your loop it should print the number of header lines for each file We might want to add one command to this loop to print out the name of the file prior to printing the number of header lines so we can more easily keep track of each file’s information. The echo command will print whatever comes after it. Try inserting this command into your loop in order to print the name of the file following by the number of header lines. Did you need to list all your files by hand? Copying and pasting a lot of names can be time consuming and error prone. Logically, you want to make your list following a particular rule, for example all of the files in this folder. We can use a wildcard (denoted *) to do this. So instead of writing out all the file names, replace all of that just with * as your list. In other situations keep in mind that we might want to be more precise. * really suggests any number of any character. To indicate all files that end in fastq.gz we could use *fastq.gz as our list. "],["tree-thinking-quiz.html", "Chapter 7 Tree thinking quiz", " Chapter 7 Tree thinking quiz Before we delve into trees it’s good to see what you are thinking so you can understand (at least partly) what you learn later. "],["target-capture-data.html", "Chapter 8 Target capture data 8.1 Background 8.2 Running HybPiper to assemble genes 8.3 Process outputs 8.4 Visual inspection of data", " Chapter 8 Target capture data 8.1 Background Much of the information in this chapter is drawn from the HybPiper tutorial, which can be found at https://github.com/mossmatters/HybPiper/wiki/Tutorial First make a folder in your home directory to hold your outputs for this component of the project. We have pre-installed the program HybPiper, which assembles target sequences from short high-throughput DNA sequencing reads. By assembling each target sequence for each species we will be able to produce a dataset that allows us to understand the evolutionary history of the species. To access HybPiper run. This activates a conda environment (don’t worry about this!). conda activate /mnt/homes4celsrs/shared/envs/hybpiper In this case, our target sequences are the Angiosperm 353 standard targets. Target capture is a standard technique for sequencing many known loci simultaneously using a library of sequences. We have provided you with low-coverage sequence data from using these target sequences as probes (your fastq files). HybPiper first assigns each read to a particular target using alignment software (BWA). These reads are then assembled using a standard genome assembler (Spades). HybPiper outputs a fasta file of the (in frame) CDS portion of each sample for each target region. 8.2 Running HybPiper to assemble genes The basic command for HybPiper is as follows: hybpiper assemble -t_dna targets.fasta -r species1_R*_test.fastq --prefix species1 --bwa --cpu 3 Notice the commands and flags. Move to the results directory in your workshop directory to run the following commands. You should run this command, with the following changes: The argument for t_dna is the fasta file in the shared workshop folder - you will need to indicate a full relative path not just the file name The argument for the the r flag is the filename(s) including paths for the fastq files of interest. Note the * indicates that we will use both pairs of the sequencing read files. The prefix should be indicative of the species name Note that we are sharing a server so we are specifying 3 cpu’s per group - if you are running on your own machine you can omit this flag and HybPiper will use all available resources. With limited shared resources this run may take a few minutes. To view your results, list the contents of the folder and folders inside this. You should see a folder for each gene. Within each folder you can see several fasta files, which you can view. To automatically run all samples consecutively you will need to loop through them. Go back and look at the prior example of loops. One approach is to make a list of all the names of the species in a file and then read that and use them one at a time. First we need to make the list. We want to automate this process because in some cases you could have a lot of samples. Additionally, copying and pasting names into a list is prone to error. Additionally, once you automate this process you could repeat it easily and quickly for other datasets. For starters make a list of just the R1 files (you don’t want R2 files because they have the same name so you would have duplicates). First cd back to your data folder. Output your list to a file in your results folder named namelist.txt by “redirecting” the output using &gt;. ls *R1* &gt; ~/evolution_workshop/results/namelist.txt Check that this worked using cat or by viewing the file and counting the number of lines.. We will need to adjust this output do remove the R1 label so that we can use wildcards to list both files together. One approach is to notice a pattern in filenames: the files start with a sample identifier followed by an underscore followed by another identifier. Thus, we can “cut off” all of the name after the second underscore and still have a unique name for our sample. We use the “cut” command to make our data into columns using _ (specify a delimiter with -d), then we select the first two of these (use the field flag -f). ls *R1* | cut -d &#39;_&#39; -f 1,2 &gt; ~/evolution_workshop/results/namelist.txt Check this worked. Now we can run HybPiper on all of our data sequentially. In the following example, the species names are entered in the namelist.txt file. The loop will iterate through them one at a time and run HybPiper. while read name do [insert command here] done &lt; [path to]/namelist.txt This analysis will take some time. Check with an instructor before running this command to ensure you have done everything correctly. Additionally, it’s advisable to run this command in the background. That means the command will run but you will continue to have access to your prompt so you can work. Additionally you can log out of the server (eg for lunch) and the command will continue to run. To run the command in background enclose the entire command in { } and then add &amp;&gt; output.log &amp; at the end to write the information that would normally print to the screen (as you saw when running a single command) to the file output.log. You can view this file to watch the progress of your command. 8.3 Process outputs To obtain some information about the results of this process use the hybpiper stats command as follows. hybpiper stats -t_dna [target file] gene [path to]/namelist.txt From https://hackmd.io/@mossmatters/HkPM7pwEK#Getting-HybPiper-Stats Hybpiper stats will generate two files, seq_lengths.tsv and hybpiper_stats.tsv. The first line of seq_lengths.tsv has the names of each gene. The second line has the length of the target gene, averaged over each “source” for that gene. The rest of the lines are the length of the sequence recovered by HybPiper for each gene. If there was no sequence for a gene, a 0 is entered. 8.3.1 Viewing information While you can cat or head this file it is difficult to view this information on the command line. However, R is excellent for reading data in this format. Click the Console tab While you could run commands here as you have in the shell, instead we will create a script to keep track of all of your commands. Note that this is also possible in the shell. Select File - New File - R script Save this file in the scripts folder Load the tidyverse library We will use the read.table command because our data are plain text separated by tabs. We have three arguments: the filename (including path), how the columns are separated, and whether the file has a header line. Note: R assumes you are in your project folder so all paths should be relative to this. stats &lt;- read.table(&quot;results/hybpiper_stats.tsv&quot;, sep = &quot;\\t&quot;, header = TRUE) View your stats data. Repeat this analysis for seq_lengths.tsv. Given your observation of the output tables, you may be interested in some of the following questions: What are the min, max, and average number of genes retrieved across samples? What is the range of lengths retrieved for a given gene? You should take a look at the data and imagine how you would calculate this. Unfortunately,this process (e.g. calculating the min value of each column individually) could be challenging to communicate to the computer effectively. Furthermore, when we look at data in this “rectangular” form, we often want to ensure that our data are “tidy”. Tidy data usually has one sample per row. Currently our data have many observations per gene per row. If we were to look at the lengths file and make a new row that includes our calculation of the standard deviation of the gene lengths this would be a row with summary information not sample information. Instead of working with these data frames directly we are going to take a look at an example that I have made for you that includes stats for just two samples in “long form.” Read the file in the shared workshop folder using the read_csv command. Take a look at how these data are organized. Can you see how the data are organized that each row contains one piece of information? Plot value v. stat as a scatter plot. We can add a couple of tweaks to make our view better. First we can specify that we want each sample to be a different color by including a color = Name argument in our aes. Second we can rotate the graph 90 degrees to better view the data and labels by adding a layer coord_flip(). We can also generate a summary table with one row per stat and information in that row including the mean, standard deviation, etc. across genes. The first component of this is to develop groups of rows by stat. I like to imagine this as drawing boxes around all rows that contain a particular stat. We use the group_by function to communicate to the computer these “boxes”. group_by(stats_example, stat) If you run this command the data won’t appear to be any different. Now we need to generate a summary table where each group is collapsed into a single row in our new table containing the stat, its average, and standard deviation. mean_stats &lt;- group_by(stats_example, stat) %&gt;% summarize(mean_across_species = mean(value), sd = sd(value)) Note that here we use %&gt;% as the pipe command to send the output of one command to the next. You have send the shell pipe | and this works the same way. In our summarize command we provide new column labels and what the contents of these columns will be. Click on your mean_stats variable to view your summary table. In order to work with your lengths table you need to know how to produce a long format table. This is tricky and will take some practice so don’t worry if it seems complicated initially. We will use the pivot_longer function. If you haven’t noticed already, when you type a function and hit the tab button on your keyboard you will see a list of arguments. The first argument you need is the data. The second argument is the list of columns that will not be in the long table and currently contain the observation data. For us this is all the columns from NumReads to GenesWithChimeraWarning. Now we need to envision our new table. Go back and look at the example table for some help here. I specified a table with a column to indicate the particular stat we are measuring and a second column for the actual value observed. This command, with these four arguments looks like the following: stats_long &lt;- pivot_longer(stats, cols = NumReads:GenesWithChimeraWarning, names_to = &#39;stat&#39;, values_to = &#39;value&#39;) Now try to work with the lengths table. First make a long format table. You want to output a table with the columns Species, gene, and length. Did you notice that the mean lengths are already included in this table? That’s really useful but also these data are not information about each sample and if we want to make summary tables and graphs we don’t want to include them. We can use the filter command to only include data that doesn’t specify MeanLength in the Species column. lengths_long_filtered &lt;- filter(lengths_long, Species != &quot;MeanLength&quot;) Now can you filter this output to include only data where the value in the length column is greater than 0. Now you should be able to make a summary table including the the mean length per gene, standard deviation of length per gene, and count of genes. Note that we did our second filtering step so these values would be accurate. Additionally, the counting function is n() (no arguments required). Save your script! 8.3.2 Obtain gene data At this point, we will go back to our Terminal. Remind yourself of your folders and data and note each sequence is in its own folder. We can use HybPiper to fetch the sequences recovered for the same gene from multiple samples and generate a file for each gene. Use the following command: hybpiper retrieve_sequences dna -t_dna [target file] --sample_names [path to]/namelist.txt You should now see one file per gene in your results folder. Each file is fasta formatted with the data for each available species. In the next section we will create an alignment of all species for each gene. Note that HybPiper has additional features we will not use in this workshop due to a lack of time. 8.4 Visual inspection of data To check that your genes assembled correctly and are likely the correct species you may use BLAST, which is found at https://blast.ncbi.nlm.nih.gov/Blast.cgi Select Nucleotide BLAST Paste one of the contigs from one of the genes from one of the species Select Somewhat similar sequences (blastn) BLAST Examine the species and the match "],["results.html", "Chapter 9 Results 9.1 Alignment 9.2 View in R 9.3 Trimal 9.4 Concatenated data analysis 9.5 Species tree analyses 9.6 Support for relationships 9.7 MrBayes 9.8 What do your trees tell you?", " Chapter 9 Results 9.1 Alignment In order to use these data to build our phylogenies, we need to align each gene. This allows our tree-building software to compare species using nucleotides that we believe share ancestry. We will use the software MAFFT. Due to the nature of this server different programs are in different “environments” so you’ll need to deactivate the hybpiper environment and activate one for mafft. conda deactivate conda activate /mnt/homes4celsrs/shared/envs/mafft Now MAFFT is available to run. However, before we align our data let’s take a look at the output. Do you remember how to print out particular lines of a file? Try printing all of the lines that start with '&gt;' in a particular FNA file. Make sure to use '&gt;' including the single quotes when searching. You should notice that HybPiper has added some information to these lines so they include more than just the name of the sample. Because these additions differ across genes subsequent analyses may treat them as separate samples. Before we do the alignment we need to remove this extra information. Again, we can think about the pattern we are looking for: we want to go through each FNA file (this should suggest using a loop) and keep the first “word” in each line (think of a word as a set of characters not separated by spaces). We use the cut command to divide each line of our file into columns. We use the -f flag to indicate the field (column) we want to keep, and the -d flag to indicate how to separate columns (i.e. with a space: ' '). An example cut command for one file could look like the following: cut -f 1 -d &#39; &#39; 7577.FNA &gt; 7577.FNA.fa Now use a loop as before to repeat this command for all FNA files. Now we can loop through each of these files and output an alignment. The basic mafft command looks like the following, assuming you replace FILE with a particular file. mafft --auto --thread 1 7577.FNA.fa &gt; 7577.FNA.fa.fasta Remember you have 350 genes (i.e. 350 fasta files) so you want to run these alignments in a loop. As before, write a loop to go through each FNA.fa file and output an alignment. 9.2 View in R The best way to view an alignment is in a specialized program, but because we have R easily available we’ll view an example here. Make a new script to run this alignment view. You should first load the ape and ggmsa library. (This uses the gg as in ggplot and msa, which stands for multiple sequence alignment.) Read in your fasta file with ape’s read.FASTA. Use the ggmsa command as follows, substituting the particular variable that contains your fasta file. Allow a little while for this to run. You may need to click the Zoom button above the plot to see it more clearly. ggmsa(seqs_7577, start = 20, end = 120, char_width = 0.5, seq_name = T, color = &quot;Chemistry_NT&quot;) + geom_msaBar() Now repeat this process for the same gene with the unaligned data. Can you see the difference? 9.3 Trimal TBD 9.4 Concatenated data analysis There are multiple ways to analyze data. The first is to concatenate everything. Make sure you are in your Terminal. conda deactivate conda activate /mnt/homes4celsrs/shared/envs/amas python3 /mnt/homes4celsrs/shared/envs/amas/bin/AMAS.py concat -f fasta \\ -d dna --out-format fasta --part-format raxml -i *FNA.fa.fasta \\ -t concatenated.fasta -p partitions.txt Note that if some data was not found for some genes you may need to delete files (use the rm command) to get AMAS to concatenate your data. 9.4.1 Building trees with IQTree We will build our first tree using our complete concatenated dataset. The program IQtree infers phylogenetic trees by maximum likelihood. This approach starts with a tree and calculates the likelihood of the data on the tree (i.e. calculating the probability of each site fitting the tree given a model of substitution and multiplying them together). We use the General Time Reversible (GTR) model to allow sites to change back and forth among different bases with particular probabilities. We also allow a Gamma (G) distribution of rates of substitution across sites. We allow partitioning of the data by gene so that different genes can evolve according to different models. Additionally we have included bootstrapping in our analysis to get a measure of support for relationships. conda deactivate conda activate /mnt/homes4celsrs/shared/envs/iqtree iqtree2 -nt 2 -s concatenated.fasta -spp partitions.txt -pre iqtree_tree -B 1000 -m GTR+G 9.4.2 Viewing trees Make a new RScript to view your tree in R. Load the libraries tidyverse and ape. Use the read.tree command and provide the path to the tree as the argument Use the plot command providing the tree variable as the argument, then add , show.node.label = TRUE 9.4.2.1 Rooting your tree Our tree-building programs create trees that are unrooted because we do not know the direction of changes among species (e.g. for a single difference we are unable to say if an A mutated to a T or vice versa). Thus, we should properly view out trees as unrooted. plot(tax1, &quot;u&quot;, show.node.label = TRUE, cex = .5) Note that I have added an argument to make the font size bit smaller so we can read all of the labels. You should adjust this value as needed and use the Zoom feature to better view your tree. However, we have created taxon sets where one of the included species is known to be more distantly related. We can use this to set the root for the tree. You should view the list of taxa for your group in the file in the shared folder for this workshop. The last taxon in your list is the outgroup. You can root your tree as in the following example using the root command and the outgroup argument. Note that your outgroup will be different if you are using a different taxon set. Additionally you must use the tip label that you can see on your tree not the labels in this spreadsheet. If you want to view the tip labels as a list use tax1$tip.label tax1_root &lt;- root(tax1, outgroup = &quot;A217_CKDN220062756-1A&quot;) Now that you have rooted your tree you can plot this new tree as before. 9.4.2.2 Relabeling your tips with species information There are a couple of ways to relabel the tips of your trees with correct species names. We will use a straightforward, manual approach. We will look at the list of tips and then manually replace them with a list of species names. You should keep in mind that this approach is prone to error and challenging for many taxa; however an automated approach is a bit more challenging to set up in this course. List the tip labels as you did in the previous step Make a list of new tip labels - it should look something like the following but with more and different species sp_names &lt;- c(&quot;Centropogon mandonis&quot;, &quot;Siphocampylus andinus&quot;, &quot;Centropogon mandonis&quot;) Assign these species names to the tip labels tax1$tip.label &lt;- sp_names If you relabeled the tips of your unrooted tree you need to root the tree with the appropriate outgroup name. You can then plot this rooted and corrected tree. 9.4.3 Tree for all data using output of individual hybpiper runs In the prior analysis we examined all of the data combined. For the next analysis we will estimate individual gene trees and then combine these into a species tree. iqtree -s concatenated.fasta -S partitions.txt -pre iqtree.loci -nt 2 -S tells IQ-TREE to infer separate trees for each partition. The output files are the same, except that now your treefile will contain a set of gene trees. For a more extensive tutorial on IQTree see http://www.iqtree.org/workshop/molevol2019 9.5 Species tree analyses An alternative is to use a species tree approach. We will use the software ASTRAL. java -jar /mnt/homes4celsrs/shared/ASTRAL/astral.5.7.8.jar -i iqtree.loci.treefile -o astral_output.tre 2&gt;astral.log -i is the flag for the input file -o is the flag for the output file 2&gt; saves the output log information The output in is Newick format and gives: the species tree topology branch lengths in coalescent units (only for internal branches or for terminal branches if that species has multiple individuals) branch supports measured as local posterior probabilities View this tree as before and compare it to the other tree you estimated using IQTree for partitioned concatenated data. More information on astral can be found at https://github.com/smirarab/ASTRAL 9.6 Support for relationships In our initial IQTree analysis we obtained bootstrap support for each node. An alternative approach are gene and site concordance factors. For more information see http://www.robertlanfear.com/blog/files/concordance_factors.html . You can compute gCF and sCF for the tree inferred under the partition model: iqtree -t iqtree_tree.treefile --gcf iqtree.loci.treefile -s concatenated.fasta --scf 100 -nt 2 -t specifies a tree –gcf specifies the gene-trees file –scf 100 to draw 100 random quartets when computing sCF. Repeat for the astral tree if it differs. View the tree (iqtree_tree.treefile.cf.tree) in R with these support values. The tree will show boostrap/gcf/scf on the nodes. 9.7 MrBayes The next approach we will use for tree building is implemented in MrBayes. We need to do some setup before we can run this software. R has some tools we can use to convert out data to the write format and add some instructions. Note that some of the following was adapted from https://gtpb.github.io/MEVR16/bayes/mb_example.html Load your data into R (use a new script for organizational purposes) library(ape) library(tidyverse) myseqs &lt;- read.dna(&quot;results/concatenated.fasta&quot;,format=&quot;fasta&quot;,as.matrix=FALSE) MrBayes requires Nexus format, with an added block giving instructions to MrBayes. We first save the data as Nexus format, and read back in to manipulate further. write.nexus.data(as.character(myseqs),&quot;results/concatenated.nex&quot;,interleaved=TRUE,gap=&quot;-&quot;,missing=&quot;N&quot;) myseqs_nex &lt;- readLines(&quot;results/concatenated.nex&quot;) Fix missing because AMAS using ? for missing myseqs_nex &lt;- gsub(&#39;\\\\?&#39;,&#39;n&#39;,myseqs_nex) First execution block mbblock1 &lt;- &quot; begin mrbayes; set autoclose=yes; &quot; Second execution block has information about partitions. We get this from the partitions file generated by AMAS but we have to do some reformatting. Read the file with read.table Get just the last column using the pull function Now you can convert this list to a string to be inserted into the output file partition_string &lt;- toString(partitions,sep = &quot;, &quot;) Finally, put all the information necessary for MrBayes into a single string mbblock2 &lt;- paste0(&quot; partition favored = &quot;,nrow(partition_file),&quot;:&quot;,partition_list,&quot;;&quot;) Add a block for the MCMC parameters. mbblock2 &lt;- &quot; mcmc ngen=10000000 nruns=2 nchains=2 samplefreq=1000; sump; sumt; end; &quot; We then paste the blocks together and write to a file. myseqs_nexus_withmb &lt;- paste(paste(myseqs_nex,collapse=&quot;\\n&quot;),mbblock1,mbblock2,sep=&quot;&quot;) write(myseqs_nexus_withmb,file=&quot;concatenated.nex.mb&quot;) Now run (on the command line) (probably in background as you did previously). ../../shared/bin/mb &quot;results/concatenated.nex.mb&quot; Before looking at your tree check the output. In some cases you may need to run your analysis for longer and a note to this effect will be toward the end of the output. Make sure to give the log file for this run a relevant name. You don’t want to overwrite the output from HybPiper and you might want to keep this output if you do a longer run. Additionally, if you rerun the analysis with additional time make sure you don’t overwrite the current tree output. You can find your tree in your results folder with the extension .con.tre. Because Mr. Bayes generates output in a particular format you will need to read it in slightly differently and convert it to the standard phylo format. mrbayes_tree &lt;- treeio::read.mrbayes(&quot;results/concatenated.nex.mb.con.tre&quot;) mrbayes_phylo &lt;- treeio::as.phylo(mrbayes_tree) probs = as_tibble(mrbayes_tree) %&gt;% pull(prob_percent) mrbayes_phylo$node.label = probs[(length(mrbayes_phylo$tip.label)+1):length(probs)] Reroot your tree Fix the names to be species Plot your tree 9.8 What do your trees tell you? "],["dates-and-evolution.html", "Chapter 10 Dates and Evolution 10.1 Dates / Times on Trees", " Chapter 10 Dates and Evolution 10.1 Dates / Times on Trees If you look at your tree, you will notice the tips don’t “line up” on the right side. It’s as if not all species made it to the present day. This is only an appearance because branch lengths are in substitutions per site and not all species have the same substitution rate. We can adjust the branch lengths to match time using the rates. Usually we have calibration dates with complex probability distributions. In this case we are taking an extremely simple approach solely for the purpose of seeing a phylogeny where the branches appear to be time. mrbayes_chronos &lt;- chronos(mrbayes_phylo) We can also generate the tree using a “strict clock model”. ### strict clock model: clock_rate &lt;- chronos.control(nb.rate.cat = 1) mrbayes_chronos_clock &lt;- chronos(mrbayes_phylo, model = &quot;discrete&quot;, control = ctrl) Now let’s plot all three trees together so you can compare the branch lengths. par(mfrow = c(3,1)) plot(mrbayes_phylo, cex = 1) plot(mrbayes_chronos, cex = 1) plot(mrbayes_chronos_clock, cex = 1) "],["understanding-the-likelihood-function.html", "Chapter 11 Understanding the likelihood function 11.1 Background 11.2 The model 11.3 The likelihood function 11.4 A mini example with a sample size of two 11.5 The bird alarm example 11.6 Where is the evidence for different behavior? 11.7 Are the freezing times different for the two groups?", " Chapter 11 Understanding the likelihood function 11.1 Background Ari Martinez spent multiple summers doing field work in the Peruvian Amazonian. In 2011, Ari observed an interesting behavior in heterospecific flocks. Depending on where on the forest a bird was feeding its response would differ. For example if a bird is feeding on the ground(dead-leaf gleaning) then the bird would “freeze” for some time, but most of the flycatchers that feed on the top of the trees wouldn’t even bother to stop. Ari started timing the freezing behavior for some of these birds and was able to collect a small sample. He would play an alarm call and then measure the time birds were freezing depending on where they were foraging. This is his sample and you will be using it to understand evidence in likelihood. bird_alarms &lt;- read_csv(&quot;../../shared/AndesWorkshop2023/birdalarms.csv&quot;) head(bird_alarms) 11.2 The model We are going to model freezing time using an exponential distribution with parameter \\(1/\\theta\\), where \\(\\theta\\) is the expected time that birds of a group freeze. What is an exponential distribution with parameter \\(1/\\theta\\)? 11.3 The likelihood function The likelihood function \\(L(\\theta;X)=P(\\theta|X)\\) is the product of the exponential density evaluated in every value of the sample observed. In order to do this calculation repeatedly we are going to make our own function in R. Just like the functions you have been using, your function is designed to do a suite of commands. This allows you to avoid copying and pasting the commands multiple times and changing all their arguments. We define likelihood.fuction using the dexp() probability function. We return the negative likelihood, which is the product of all probabilities derived from our observations. Once you have defined this function and know what it does, you do not have to think about how to calculated likelihoods again. likelihood.function &lt;- function(parameter, observations){ probabilities &lt;- dexp(observations, rate=1/parameter,log=FALSE) L &lt;- prod(probabilities) return(-L) } The likelihood function is much more difficult depending on the model we select. We will discuss more about its construction when we discuss substitution models. However the way to think about it is always as the probability of the data given a model (or set of parameters) \\(P(Data|Model)\\). Knowing this definition of the likelihood then let’s ask What is the input of the likelihood function? What is the output of the likelihood function? ( and why is it negative?) 11.4 A mini example with a sample size of two If we have a sample size of two birds freezing responses \\(X=2,10\\) the maximum likelihood estimate is the average \\(\\hat{\\theta}=\\sum_{i=1}^n x_i/n\\). Therefore (mle&lt;-(2+10)/2) Note that by putting the whole command in () we both assign the output to the variable and print it. However, most of the time in difficult likelihoods it is impossible to calculate exactly who the mle so we do it numerically. (likelihood.optimization &lt;- optimize(f=likelihood.function, interval=c(0,10), observations=c(2,10))) The function optimize is used when we have unidimensional functions (one parameter). For multidimensional we use optim or even better nloptr from the package with the same name. What are the outputs? 11.5 The bird alarm example In Ari’s example we have the dead-leaf gleaning species dl.forager &lt;- bird_alarms %&gt;% filter(Forage == &quot;DL&quot;) %&gt;% pull(Response) and the flycatchers f.forager &lt;- bird_alarms %&gt;% filter(Forage ==&quot;F&quot;) %&gt;% pull(Response) #11 11.5.1 Calculate the MLE for DL and F and the likelihood value evaluated at the MLE You should be getting approximately the following values: dl.optimization &lt;- optimize(f=likelihood.function, interval=c(10,30), observations=dl.forager) (mle.dl &lt;- dl.optimization$minimum) (likelihoodval.dl &lt;- -dl.optimization$objective) (mle.f &lt;- mean(f.forager)) (likelihoodval.f &lt;- likelihood.function(mle.f,observations=f.forager)) So, are the flycatchers behaving differently than the dead-leaf gleaners? What is the evidence? Can we compare these likelihoods or MLEs? 11.6 Where is the evidence for different behavior? Likelihood functions are not only about the maximum likelihood estimate. Likelihoods represent plausibility. This is represented in the full likelihood function so it is important to explore it. We select a series of parameters and measure their plausibility for example in the interval \\((0,50)\\) parameter.vals &lt;- seq(0.0001,50,0.01) #creating an interval for possible values for the likelihood long &lt;- length(parameter.vals) # Evaluating the likelihood for each of those values p.likelihoodf &lt;- rep(0,long) for (i in 1:long){ p.likelihoodf[i]&lt;- -likelihood.function(parameter.vals[i],observations=f.forager) # Remember is negative so we need to add a sign } In different studies likelihood can be represented using different scales par(mfrow=c(1,3)) # Straight likelihood function plot(parameter.vals, p.likelihoodf, type=&quot;l&quot;,main=&quot;Likelihood for flycatchers&quot;,xlab=expression(theta),ylab=&quot;Likelihood&quot;, lwd=2,xlim=c(0,5)) #log-likelihood function, most commonly used plot(parameter.vals, log(p.likelihoodf), type=&quot;l&quot;,main=&quot;log-likelihood for flycatchers&quot;,xlab=expression(theta),ylab=&quot;Likelihood&quot;,lwd=2,xlim=c(0,5)) # Relative likelihood: Likelihood divided by the likelihood value at the MLE plot(parameter.vals, p.likelihoodf/likelihoodval.f, type=&quot;l&quot;,main=&quot;Relative likelihood for flycatchers&quot;,xlab=expression(theta),ylab=&quot;Likelihood&quot;,lwd=2,xlim=c(0,5)) What do you think about the sample size for flycatchers? 11.6.1 Plot the likelihood for dead-leaf gleaners It should look like this p.likelihooddl&lt;-rep(0,long) for (i in 1:long){ p.likelihooddl[i]&lt;- -likelihood.function(parameter.vals[i],observations=dl.forager) } par(mfrow=c(1,3)) plot(parameter.vals, p.likelihooddl, type=&quot;l&quot;,main=&quot;Likelihood for flycatchers&quot;,xlab=&quot;Rate Parameter&quot;,ylab=&quot;Likelihood&quot;,lty=2,col=&quot;red&quot;,lwd=2) plot(parameter.vals, log(p.likelihooddl), type=&quot;l&quot;,main=&quot;log-likelihood for flycatchers&quot;,xlab=&quot;Rate Parameter&quot;,ylab=&quot;Likelihood&quot;,lty=2,col=&quot;red&quot;,lwd=2) plot(parameter.vals, p.likelihooddl/likelihoodval.dl, type=&quot;l&quot;,main=&quot;Relative likelihood for flycatchers&quot;,xlab=&quot;Rate Parameter&quot;,ylab=&quot;Likelihood&quot;,lty=2,col=&quot;red&quot;,lwd=2) 11.7 Are the freezing times different for the two groups? Usually you will go ahead and do some statistical test to say “I reject that the average freezing time of the groups is the same”. Except that you can’t do a T-test (not normal), sample sizes are really small (so not so much power). The evidence of likelihood comes to the rescue. Using relative likelihoods we can compare the evidence between the two groups par(mfrow=c(1,1)) plot(parameter.vals, p.likelihoodf/likelihoodval.f, type=&quot;l&quot;,main=&quot;Evidence for responses&quot;,xlab=&quot;Rate Parameter&quot;,ylab=&quot;Likelihood&quot;) lines(parameter.vals, p.likelihooddl/likelihoodval.dl,lty=2,col=&quot;red&quot;,lwd=2) legend(x=35,y=0.8, col=c(&quot;black&quot;,&quot;red&quot;),legend=c(&quot;flycatcher&quot;,&quot;dead-leaf&quot;),lty=1:2) "],["what-is-bayesian-statistics.html", "Chapter 12 What is Bayesian statistics? 12.1 What about all the fighting of “frequentists” vs. “Bayesians” 12.2 When do we choose Bayesian statistics? 12.3 Basic concepts of Bayesian Statistics 12.4 Example: Online dating 12.5 The likelihood function 12.6 The update! The posterior distribution 12.7 Optional section: Doing your first MCMC 12.8 Changing the proposal", " Chapter 12 What is Bayesian statistics? Bayesian statistics is a branch of statistics that focuses in inferring and forecasting the probability of events making sure we account for uncertainty. There are two central differences from traditional (frequentist) statistics The way Bayesians define probabilities Parameters are unknown but random and we need to measure their uncertainty. At all times we need to make sure we think about these two key issues. 12.1 What about all the fighting of “frequentists” vs. “Bayesians” From a practical point of view all the fighting is just a series of bad misunderstandings. Frequentist statistics has a long history of fighting that started early with the introduction of a concept that we have studied likelihood function. Interestingly, people confuse the likelihood function with frequentist statistics, and now people host talks arguing that frequentist (or likelihood statistics) are bad even though likelihood can have the same inference potential than Bayesian statistics. 12.2 When do we choose Bayesian statistics? In Biology we choose Bayesian statistics because they have a powerful computational tool that is called MCMC that stands for Monte Carlo Markov Chain optimization. This tool is extremely easy to implement in new software and deals with very complex models quickly. It is so powerful that an MCMC can give us estimates of many parameters without optimization nightmares. Today we will explore this power. 12.3 Basic concepts of Bayesian Statistics Bayesian statistics inherit their from the Bayes’ Theorem. We have two events \\(A\\) and \\(B\\) so the conditional probability of \\(A\\) given \\(B\\) is defined as \\[P(A|B)=\\frac{P(AB)}{P(B)}\\] or applying twice Bayes’ Theorem we get \\[ P(A|B)=\\frac{P(B|A)P(A)}{P(B)}\\] 12.4 Example: Online dating Dating online can be daunting for introverts. An introvert, lets call them Felix (F), asks a person Pepa (P) out for dinner, and Pepa accepts. F is super nervous and thinking about these two events A: is the event that P is into F B: is the event that represents a date between F and P going well The probability \\(P(A)\\) represents a priori how much F believes P is into them. For example, \\(P(A)=0.1\\) represents lack of confidence, meaning F doesn’t believe P is that interested in them. An overly confident person can have a \\(P(A)=0.9\\). However, F knows how to play the long game. They are interested in \\(P(A|B)\\) meaning, after observing how well and fun their date goes, F can update their expectations, that is, if the dinner goes well, maybe they increased their chances of dating for longer P. 12.4.1 \\(P(A|B)\\) is difficult to estimate How do we update the probability of P liking F given how the date went? It is always easier to think on the opposite conditional: \\(P(B|A)\\): Given that P is into F the probability of dinner going well is… and we can go beyond this. We can think on the complement of event \\(A\\) denoted as \\(A^C\\) that is the event that P is NOT into F. In probability world the rule is that the complement has a probability \\[P(A^C)=1-P(A)\\] (the probability of P is not into F is one minus the probability of P liking F, hence complementary probabilities) 12.4.2 Coming back to the initial idea of how we define Bayesian statistics The way Bayesians define probabilities- It is all about betting. There is no great way to a priori decide what \\(P(A)\\) (the probability of a person liking me). This is subjective and up to the modeler. This is why we want to be careful with prior distributions. 12.4.3 The random variable To become a true Bayesian and be able to infer how dating is going we need to move from the space of events A and B to the space of numbers using an important concept: random variables. Random variables represent a function that takes us from an event to a number. Let’s define our first one \\(A\\): The event that Pepa likes Felix. \\(X\\): random variable (a function that takes us from an event to a number). Define as \\[X= \\begin{cases} 0 &amp; \\textrm{when } A^C \\textrm{ happens}\\\\ 1 &amp; \\textrm{when } A \\textrm{ happens} \\end{cases}\\] So now instead of using the probability \\(P(A)\\) we can use instead \\(P(X=1)\\) and both mean the same. 12.4.4 The prior distribution Now suppose that \\(P(X=1)\\) is redefined using a parameter called \\(\\theta\\) (theta) that is between 0 and 1, and this also means that \\(P(X=0)=1-\\theta\\). Just like Felix declared his shyness initially \\(\\theta=0.1\\). That would be all well if you are completely certain, but as any person in the world, sometimes a person likes you and sometimes they don’t. So it would be much better to assume \\(\\theta\\) (parameter) is uncertain and try to model it. For that we are going to use a probability distribution called Beta. A beta distribution is convenient because it goes from (0, 1). You could choose any other distribution you wish as long as it makes \\(\\theta\\) go between (0,1). Beta has two parameters a and b and the mean of the distribution is a/(a+b). Assign the value 0.1 to the variable theta, 2 to the variable a, and 5 to the variable b. But let’s explore what a Beta distribution does. Create a function that takes theta, a, and b as arguments. The function should return dbeta(theta, a, b). Load the library tidyverse. Create a table of values a and b so we can see the different shapes of Beta distribution length &lt;- 1e4 d &lt;- crossing(shape1 = c(.1, 1:4), shape2 = c(.1, 1:4)) %&gt;% expand(nesting(shape1, shape2), x = seq(from = 0, to = 1, length.out = length)) %&gt;% mutate(a = str_c(&quot;a =&quot;, shape1), b = str_c(&quot;b =&quot;, shape2), group = rep(1:length, each = 25)) Use the head function to see what is in the table Now plot different Beta distributions d %&gt;% ggplot(aes(x = x, group = group)) + geom_line(aes(y = dbeta(x, shape1 = shape1, shape2 = shape2)), color = &quot;grey50&quot;, size = 1.25) + scale_x_continuous(breaks = c(0, .5, 1)) +coord_cartesian(ylim = c(0,3))+ labs(x = expression(theta), y = expression(paste(&quot;p(&quot;, theta, &quot;)&quot;))) + theme(panel.grid = element_blank()) + facet_grid(b~a) Let’s interpret the Beta distributions. These are potential prior distributions, they are subjective and represent different beliefs that we have about the statisticians. Here, we are focusing on the second central idea of Bayesian statistics: Parameters are unknown but random and we need to measure their uncertainty. We do this by assuming the parameter \\(\\theta\\) has a prior distribution \\(Beta(a,b)\\). 12.4.5 The Data and their distribution In our case, Felix goes out for one dinner (or more) trying to figure out if Pepa is interested. Let’s assume a crazy Netflix show scenario (like “Love is Blind”- don’t judge me!). Felix and Pepa decide to date for exactly three dates \\(N=3\\). How can we model this? \\[Y \\textrm{ is the random variable of the number of dates that went well so the values } Y=0,1,2,...,N \\textrm{ are our only possibilities}\\] The probability of the number of dates that went well is going to be dependent on the likability of the statistician \\(\\theta\\). We can model this using a Binomial distribution \\[P(Y=k|\\theta)= {N\\choose k} \\theta^k (1-\\theta)^{N-k}\\] Let’s see what this Binomial distribution looks like # install.packages(grid) library(grid) date_number &lt;- 0:3 theta &lt;- 0.1 df &lt;- data.frame(x = date_number, y = dbinom(date_number, 3, 0.1)) p1 &lt;- ggplot(df, aes(x = x, y = y)) + geom_bar(stat = &quot;identity&quot;, col = &quot;hotpink&quot;, fill = &quot;hotpink&quot;) + scale_y_continuous(expand = c(0.01, 0)) + xlab(&quot;x&quot;) + ylab(&quot;Density&quot;) + labs(title = &quot;dbinom(x, 20, 0.5)&quot;) + theme_bw(16, &quot;serif&quot;) + theme(plot.title = element_text(size = rel(1.2), vjust = 1.5))+labs(title=&quot;Binomial distribution&quot;, x =&quot;Number of dates that went well&quot;, y = &quot;Probability&quot;) p1 12.5 The likelihood function Likelihood approaches have gotten a bad rap over the years. The likelihood function often gets bunked as a “frequentist” approach. This is incorrect, likelihood functions are extremely useful depending the problem, and they are better understood if we think of them as the function of evidence (discussion to come). Let’s say the three dates are completed and 2 out of the 3 went well therefore the probability of 2 dates going well is \\[P(Y=2|\\theta)= {3\\choose 2} \\theta^2 (1-\\theta)^1\\] If \\(\\theta=0.1\\) then \\(P(Y=2|\\theta=0.1)= {3\\choose 2} 0.1^2 (0.9)^1=0.027\\). So if Felix is not a likable person the probability of two dates going well is really low 2.7%. This is an unlikely scenario (you see where the word likelihood comes from now?). Now, let’s think that this Netflix show goes crazier and Felix has to date two people three times. With the first person they have \\(y_1=1\\) good date and with the second \\(y_2=2\\) good dates. Then the likelihood function in this situation is \\[P(Y=y_1|\\theta)\\times P(Y=y_2|\\theta) = {3\\choose 1} \\theta^1 (1-\\theta)^{2}\\times {3\\choose 2} \\theta^2 (1-\\theta)^{1} \\approx \\theta^{1+2}(1-\\theta)^{2+1}=\\theta^3(1-\\theta)^3\\] Formally, the likelihood function is the probability of the data given the model, and that is a product of the individual probabilities of each couple dating. \\[P(Data|\\theta)= \\prod_{y_i=1}^{2}P(Y=y_i|\\theta)\\approx \\theta^3(1-\\theta)^3\\] If you didn’t have any information a priori about \\(\\theta\\), what would you say about it after observing that in the first three dates two went well, and in the second three dates only one went well? binomial_likelihood &lt;- function(theta, data,n) { # `theta` = success probability parameter ranging from 0 to 1 # `data` = the vector of data (i.e., a series of 0s and 1s) long &lt;- length(theta) z &lt;- rep(0,long) for(i in 1:long){ prob.vect &lt;- sapply(data,FUN=dbinom, size=n, prob=theta[i]) z[i] &lt;- prod(prob.vect) } return(z) } observed_dates &lt;- c(2,1) binomial_likelihood(theta=c(0.1,0.3), data=observed_dates,n=3) 12.6 The update! The posterior distribution The posterior distribution then is the updated probability of likability of Felix. After going for some dates, they should have a better way to say “I’m a nice person to go out with”. That’s why Bayesians refer to posterior distributions as updated probabilities. The posterior distribution is proportional to the likelihood multiplied by a prior distribution for the model \\[\\underbrace{P(\\theta|Data)}_{Posterior} \\propto \\underbrace{P(Data|\\theta)}_{Likelihood} \\times \\underbrace{P(\\theta)}_{Prior}\\] Remember the proportional part comes from ignoring in the Bayes’ Rule the denominator .That is, we ignore the probability of the data aggregated for all the possible models for \\(\\theta\\) that is \\(P(\\theta)\\). Okay so let’s calculate the posterior distribution \\[P(\\theta|Data) \\propto \\underbrace{\\theta^k(1-\\theta)^{n-k}}_{\\textrm{Likelihood (product of binomials)}} \\times \\underbrace{\\theta^{a-1}(1-\\theta)^{b-1}}_{\\textrm{prior Beta distribution}}\\] This is a horrible object, how do we deal with this? how do we know how the posterior looks like? trial_data &lt;- c(1,2) number_dates &lt;- 3 a &lt;- 2 b &lt;- 5 d &lt;- tibble(theta0 = seq(from = 0, to = 1, length.out = 100)) %&gt;% mutate(`Prior (beta)` = dbeta(theta0, shape1 = a, shape2 = b), `Likelihood (Binomial)` = binomial_likelihood(theta = theta0, data=trial_data,n=number_dates), `Posterior (beta)` = dbeta(theta0, shape1 = 4, shape2 = 7)) glimpse(d) d %&gt;% gather(key, value, -theta0) %&gt;% mutate(key = factor(key, levels = c(&quot;Prior (beta)&quot;, &quot;Likelihood (Binomial)&quot;, &quot;Posterior (beta)&quot;))) %&gt;% ggplot(aes(x = theta0)) + # densities geom_ribbon(aes(ymin = 0, ymax = value), fill = &quot;grey67&quot;) + labs(x = expression(theta), y = NULL) + facet_wrap(~key, scales = &quot;free_y&quot;, ncol = 1) + theme(panel.grid = element_blank()) The goal in any Bayesian statistics inference is to get the posterior distribution and this is done using a computational algorithm called the MCMC- Markov Chain Monte Carlo that allows us to propose values for \\(/theta\\) from and accept of reject them depending on the odds that the data is likely under those values. 12.7 Optional section: Doing your first MCMC The MCMC (Markov Chain Monte Carlo) algorithm allow us to characterize and optimize the posterior distributions of all the parameters of interest. The MCMC you will be doing today is a Metropolis-Hastings, it is a rejection algorithm, it proposes a value for the parameter \\(\\theta\\) and then asks: Does that value improves the posterior probability? If yes then accept it other wise, go back and propose something new. Hands on! 12.7.1 The proposal function We are going to propose a \\(\\theta_{new}\\) proposalfunction &lt;- function(nvals=1){ unif_val &lt;- runif(nvals,min=0, max=1) return(unif_val) } #Select randomly a value for theta from a uniform distribution 12.7.2 The Metropolis-Hastings This is the rejection algorithm that we want. We are going to select a starting value for theta our search startvalue and a number of times we want to keep searching for new values iterations the number of times we search will depend completely on whether we achieve convergence (more about convergence to follow). a &lt;- 2 b &lt;- 5 run_metropolis_MCMC &lt;- function(startvalue, iterations){ chain = rep (0,iterations+1) chain[1] = startvalue # our algorithm starts here with a set of start values for (i in 1:iterations){ theta_old&lt;-chain[i] theta_new = proposalfunction(1) # We propose to move somewhere else, utilizing as the mean of our normal distribution for the proposal the current values we have odds = (binomial_likelihood(theta_new,observed_dates,n=3)*prior_distribution(theta_new, a,b))/(binomial_likelihood(theta_old,observed_dates,n=3)*prior_distribution(theta_old, a,b)) if (runif(1) &lt; odds){ #if that difference in probability is really big, then awesome we are exploring better the posterior and we should take the proposed values chain[i+1] = theta_new }else{ # not great, better stay where we are and not move, in the next round we will propose other values maybe better ones. chain[i+1] = theta_old } } return(chain) } # Test case do only one step, what happened? Compare amongst your classmates startvalue = 0.3 # Here is the start value chain = run_metropolis_MCMC(startvalue, 1) # Do one step, what happened? chain startvalue = 0.3 # Here is the start value iter=100 chain = run_metropolis_MCMC(startvalue, iter) # Do 10,000 steps head(chain) 12.7.3 Visualizing the MCMC as a way to find out convergence mcmc &lt;- data.frame(iterations=seq(1,iter+1,1),chain) # Plot ggplot(mcmc, aes(x=iterations, y=chain)) + geom_line(color=&quot;hotpink&quot;)+ labs(title=&quot;MCMC run&quot;,x=&quot;Iterations&quot;, y=&quot;Posterior distribution&quot;) hist(chain,col=&quot;hotpink&quot;) acceptance = 1-mean(duplicated(chain)) # The proportion of accepted moves, how did it go? good or bad acceptance 12.8 Changing the proposal proposalfunction2 &lt;- function(nvals=1){ beta_val&lt;-rbeta(nvals, shape1=0.1, shape2=0.1) # Beta that looks like a U return(beta_val) } a &lt;- 2 b&lt;-5 run_metropolis_MCMC2 &lt;- function(startvalue, iterations){ chain = rep (0,iterations+1) chain[1] = startvalue # our algorithm starts here with a set of start values for (i in 1:iterations){ theta_old&lt;-chain[i] theta_new = proposalfunction2(1) # We propose to move somewhere else, utilizing as the mean of our normal distribution for the proposal the current values we have odds = (binomial_likelihood(theta_new,observed_dates,n=3)*prior_distribution(theta_new, a,b))/(binomial_likelihood(theta_old,observed_dates,n=3)*prior_distribution(theta_old, a,b)) if (runif(1) &lt; odds){ #if that difference in probability is really big, then awesome we are exploring better the posterior and we should take the proposed values chain[i+1] = theta_new }else{ # not great, better stay where we are and not move, in the next round we will propose other values maybe better ones. chain[i+1] = theta_old } } return(chain) } startvalue = 0.3 # Here is the start value iter=100 chain = run_metropolis_MCMC2(startvalue, iter) # Do 10,000 steps mcmc &lt;- data.frame(iterations=seq(1,iter+1,1),chain) # Plot ggplot(mcmc, aes(x=iterations, y=chain)) + geom_line(color=&quot;hotpink&quot;)+ labs(title=&quot;MCMC run&quot;,x=&quot;Iterations&quot;, y=&quot;Posterior distribution&quot;) hist(chain,col=&quot;hotpink&quot;) acceptance = 1-mean(duplicated(chain)) # The proportion of accepted moves, how did it go? good or bad acceptance "],["introduction-to-macroevolution-modeling.html", "Chapter 13 Introduction to macroevolution modeling 13.1 Reading dataset and phylogenetic tree 13.2 Plotting a phylogenetic tree with data 13.3 Introduction to Brownian Motion 13.4 Brownian motion in phylogenetic comparative methods 13.5 Shared ancestry and continuous character evolution", " Chapter 13 Introduction to macroevolution modeling In this section, we will work on modeling a continuous trait and understanding what Brownian motion is. Start by loading the libraries ape, geiger, phytools, nlme, and phylolm. 13.1 Reading dataset and phylogenetic tree Data is a simplify version of the data and models we published in Landis, J.B., Bell, C.D., Hernandez, M., Zenil-Ferguson, R., McCarthy, E.W., Soltis, D.E. and Soltis, P.S., 2018. Evolution of floral traits and impact of reproductive mode on diversification in the phlox family (Polemoniaceae).Molecular phylogenetics and evolution, 127, pp.878-890. We have a tree with 165 tips from the Polimoniaceae family (Phlox). They grow in California. We are interested in the rate of evolution of flower length. Read in the pole_tree.nex file from the shared folder using the read.nexus function. Read the dataset of traits and pollinators visiting the flowers on the tips of the tree. This file is pole_dataset.csv in the shared folder. For many phylogenetic models we need to make sure that the row names match the tip labels on the tree. Assign the tip labels of the pole_tree dataset to rownames(pole_dataset). View the dataset. This assumes that the dataset is already in the same order as the tree file. You should see columns for Length- flower length (Response) P1- butterflies visit or not (Predictor) P2- hawkmoths visit or not (Predictor) P3- bee flies visit or not (Predictor) P4- bees visit or not (Predictor) 13.2 Plotting a phylogenetic tree with data Plot this tree An easy way to visualize phylogenies that are small is by plotting them in a rectangular fashion as we have done, but when you have a lot of tips, circular phylogenies can be more useful. You can use the type argument to plot to change the plot style. plot(pole_tree,cex=0.2,type=&quot;fan&quot;) 13.2.1 Plotting continuous data Jacob Landis measured the flower width and length and observed which pollinators come visit the flowers. Jacob was interested in understanding if we can predict flower shape based on pollinator type. For example, flowers that hummingbirds visit might be longer and narrower than flowers that are visited by bees. I wanted to create a model where the response is the flower shape and the predictor is what type of pollinator visits the flower. lengthplot &lt;- setNames(pole_dataset$Length,rownames(pole_dataset)) Stop to see what this line of code is doing. plotTree.barplot(tree=pole_tree, x=lengthplot, args.plotTree=list(fsize=0.2,ftype=&quot;i&quot;), args.barplot=list(xlab=&quot;Flower length (cm)&quot;)) Stop and see - what do you notice? Practice your tree-thinking skills. 13.2.2 Plotting discrete and continuous data We would like to show on this plot which flowers are hummingbird pollinated. We can do this by creating a new column in the data and we’ll put a color in it based on the information in column P1. pole_dataset$mycolors &lt;- ifelse(pole_dataset$P1==1, &quot;red&quot;, &quot;blue&quot;) Now plot the tree plotTree.barplot(tree=pole_tree, x=lengthplot, args.plotTree=list(ftype=&quot;off&quot;), args.barplot=list(col=pole_dataset$mycolors,border=pole_dataset$mycolors,xlab=&quot;&quot;)) legend(x=3, y=100,legend=c(&quot;Hummingbird pollinated&quot;,&quot;Other pollination&quot;), pch=22, cex=0.5, pt.bg=c(&quot;red&quot;,&quot;blue&quot;), box.col=&quot;transparent&quot;) Stop and see, what kind of (macroevolution) questions can you ask after seeing this? 13.3 Introduction to Brownian Motion Probability approaches have proven really useful to recreate evolutionary history across species. Although some of these models can be simplistic, they can be useful to establish some basic scenarios at which traits can evolve. One of these simple, yet important models is Brownian motion (sometimes called Wiener process). The movement itself was first described by a botanist(!!!) Robert Brown (1827) who wanted to describe how pollen of Clarkia pulchella moved in water. The mathematical model was actually described by Albert Einstein in 1905. Brownian motion is the result of many small forces that are added to create movement. My postdoc advisor Luke Harmon, says that it is hard for him to imagine pollen in fluid so he uses the analogy of a ball being bounced around in a large stadium as a great visual for using Brownian motion model (BM) to model the movement of the ball. It is the many small forces and directions of the people in the stadium which ultimately moves the ball all across the place. Mathematically, Brownian motion is a stochastic process. This means that it is a probability model that occurs as time advances. We denote stochastic processes as \\({X(t): t\\geq 0}\\), meaning a random variable over time, when time is greater or equal than zero. Brownian motion has three important properties: The expected value of the process as time advances is the same value at which the process started. In mathematical terms \\(E[X(t)]=X(0).\\) In each successive and non-overlapping time interval the process is independent. This iproperty is what we call in probability a Markovian property. In mathematical terms, we have two intervals of time \\((0,t)\\) and \\((t, t+s)\\), then \\([X(t)-X(0)]\\) is independent from \\([X(t+s)-X(t))]\\). The process at time \\(t\\), that is \\(X(t)\\) has a Normal probability distribution. This normal probability distribution has as mean value, the value at which the process started and a variance that is a function of time. In mathematical terms \\(X(t)\\sim N(X(0),\\sigma^2t).\\) 13.4 Brownian motion in phylogenetic comparative methods In phylogenetic comparative methods, we are assuming then that an observation of a phenotype (trait) for species A \\(x_A\\) is then representative of the average of the population after some time. Again, this is an assumption, and as every assumption, we should question this when it comes to real data. There are many traits that could be model as BM, think about body mass, brain size, foot length, all those continuous. How appropriate is to model them as BM, totally depends on the scale, and how much we believe that the trait “randomly wanders” through time. There exists other models who put a limit on how much the trait wanders, or change rapidly away from having the same expected value altogether. For the purposes of this lecture, we are interested then in understanding the evolution of trait \\(X(t)\\) (notice is a random variable over time) for two species for which we have observed values of the trait \\((x_A,x_B)\\). These two species are not independent, they have a shared evolutionary history dictated by a phylogenetic tree. This is a key point, in the past, all of your samples have been INDEPENDENT and identically distributed. The independent assumption is gone, so how do we go about acknowledging the shared evolutionary history? Using BM model we know that \\[x_A\\sim N(\\mu_0, \\sigma^2 (t_1+t_2))\\] and that \\[x_B\\sim N(\\mu_0,\\sigma^2(t_1+t_3))\\] with \\(\\mu_0=x(0)\\). Since \\((x_A, x_B)\\) are not independent from each other and share branch \\((t_1)\\) in the phylogeny we can think of each value in the tips as the result of a the sum of two evolutionary changes 1. \\(x_A=\\Delta x_1+\\Delta x_2\\) (the change of trait value during \\(t_1\\) plus the change in trait value during the \\(t_2\\) branch) 2. \\(x_B=\\Delta x_1+\\Delta x_3\\) (the change of trait value during \\(t_1\\) plus the change in trait value during the \\(t_3\\) branch) Because of the Markovian property of BM (property 2 in the introduction). We know that the change \\(\\delta x_2\\) is independent from the change in \\(\\delta x_1\\). Also, the only shared change for the two species is whatever happened during \\(t_1\\). Therefore, the shared evolutionary history for the trait for species A and B is \\[cov(x_A, x_B)= var(\\Delta x_1)=\\sigma^2 t_1\\] This covariance equation is the most important and key result used in macroevolution. Extensions of these covariance represent other popular models of continuous variables (e.g. OU, EB, Lévy). 13.5 Shared ancestry and continuous character evolution Calculate the C matrix using the vcvPhylo() function C &lt;- vcvPhylo(pole_tree, anc.nodes=FALSE) What is this the covaraince matrix of? How to interpret variance terms? Check the dimensions. What is this matrix C composed of? Why is it called vcvPhylo? 13.5.1 Always visualize your data flower.length &lt;- pole_dataset$Length names(flower.length) &lt;- pole_tree$tip.label hist(flower.length) What do you see here? do you think Brownian motion could be a good model for evolution for flower length? Sometimes to better model BM in traits that can only be positive we often log-transform. log.flowerlength &lt;- log(flower.length) hist(log.flowerlength) However, it is much harder to interpret log-scale. 13.5.2 Fitting BM on flower length We are interested in calculating the the amount of evolution using Brownian motion for flower length. We will do this with the function fitContinuous() from package geiger. fitted.BM &lt;- fitContinuous(pole_tree,flower.length, model=&quot;BM&quot;) fitted.BM Stop and Interpret the output of fitContinuous Type fitted.BM$opt - explore a little bit what this object has What is fitted.BM$opt$z0 telling us about the Brownian motion process? What is fitted.BM$opt$sigsq telling us about the Brownian motion process? What is the log-likelihood? If you assume no phylogenetic tree and complete independence in the data, what happens with the parameters? fitted.Normal &lt;- fitContinuous(pole_tree,flower.length, model=&quot;white&quot;) fitted.Normal Which model is better? (Hint: Think about model selection, but also argue about the importance of considering the phylogeny) "],["phylogenetic-generalized-linear-models.html", "Chapter 14 Phylogenetic Generalized Linear Models 14.1 Phylogenetic Linear Models 14.2 What happens if the model of evolution is not Brownian motion but something else?", " Chapter 14 Phylogenetic Generalized Linear Models Load the same libraries and datasets as in the prior chapter. 14.1 Phylogenetic Linear Models In our phylogenetic linear model we create an important macroevolution hypothesis: Flower length is determined by the types of pollinators that visit the flower. EXTREMELY IMPORTANT HERE = The main assumption of these models is that the ERRORS (not the response variable) have a normal distribution with a vector of zeros as the mean and variance-covariance matrix sigma^2 *C. Remember C is determined by the structure of the tree as we d calculated it in the continuous trait (Brownian motion) tutorial. 14.1.1 Testing the hypothesis How would you frame then a biological hypothesis from these observations? Before typing a model it would be great to think about what we are trying to achieve. #not log.flowerlength? plm.flowerlength &lt;- phylolm(formula, data=, phy=, model=&quot;BM&quot;) ## interpret this model, what is happening? what is significant and how summary(plm.flowerlength) # What is happening with the residuals of this linear model hist(plm.flowerlength$res) qqnorm(plm.flowerlength$res) Now what happens if the predictor is log(Length)? #Complete the model plm.flowerlength2 &lt;- phylolm(formula, data=, phy=, model=&quot;BM&quot;) summary(plm.flowerlength2) ## interpret this model, what is happening? what is significant and how hist(plm.flowerlength2$res) qqnorm(plm.flowerlength2$res) 14.2 What happens if the model of evolution is not Brownian motion but something else? plm.flowerlength3&lt;-phylolm(Length~ P1+P2+P3+ P4, data=pole_dataset, phy=pole_tree, model=&quot;OUrandomRoot&quot;) #What is this model? summary(plm.flowerlength3) ## interpret this model, what is happening? what is significant and how plot(plm.flowerlength3) hist(plm.flowerlength3$res) qqnorm(plm.flowerlength3$res) Overall which is the best model and why? Argue statistically but also think about flower length. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
